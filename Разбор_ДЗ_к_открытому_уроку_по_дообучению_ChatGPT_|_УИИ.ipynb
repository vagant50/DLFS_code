{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vagant50/DLFS_code/blob/master/%D0%A0%D0%B0%D0%B7%D0%B1%D0%BE%D1%80_%D0%94%D0%97_%D0%BA_%D0%BE%D1%82%D0%BA%D1%80%D1%8B%D1%82%D0%BE%D0%BC%D1%83_%D1%83%D1%80%D0%BE%D0%BA%D1%83_%D0%BF%D0%BE_%D0%B4%D0%BE%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8E_ChatGPT_%7C_%D0%A3%D0%98%D0%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На основе базы знаний компании \"Островок\" сделайте нейро-консультанта для ответов на вопросы по бронированиям. Подставьте следующие документы в код, как это было сделано в учебных материалах\n",
        "Документ с базой знаний можно найти тут: https://docs.google.com/document/d/1K8kIpWxUX8GNBtmPnStYy83zpV8zfVeBqWVVlH7wNdU\n",
        "Промпт можно найти тут: https://docs.google.com/document/d/1JCp4OWP22EZTU-HllyanhYfNSy_RwJ1aP7dnXPY2hqE\n"
      ],
      "metadata": {
        "id": "yLOgwOn4NKzK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ddRev1OMxCG",
        "outputId": "55bab661-9b51-4c16-d434-20fca3f91e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.8 yarl-1.9.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.1/965.1 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m937.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip -q install --upgrade tiktoken\n",
        "!pip -q install langchain openai chromadb\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "import requests\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import gspread\n",
        "import re\n",
        "# игнорирование предупреждений\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import logging\n",
        "logging.getLogger(\"langchain.text_splitter\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n",
        "from getpass import getpass\n",
        "\n",
        "import openai\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Запрашивает у пользователя ключ API и сохраняет его в переменную\n",
        "api_key = getpass('Введите ваш ключ API:')\n",
        "\n",
        "# Устанавливаем ключ API напрямую через openai.api_key\n",
        "openai.api_key = api_key\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIZn8ED3OBua",
        "outputId": "76090b21-2818-4dd7-e41a-b4b86cb7bf0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите ваш ключ API:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_document_text(url: str) -> str:\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    text = response.text\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_search_index(text: str) -> Chroma:\n",
        "    return create_embedding(text)\n",
        "\n",
        "\n",
        "def create_embedding(data):\n",
        "    def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "        \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        num_tokens = len(encoding.encode(string))\n",
        "        return num_tokens\n",
        "\n",
        "    source_chunks = []\n",
        "    splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1024, chunk_overlap=0)\n",
        "\n",
        "    for chunk in splitter.split_text(data):\n",
        "        source_chunks.append(Document(page_content=chunk, metadata={}))\n",
        "\n",
        "    # Создание индексов документа\n",
        "    search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings(openai_api_key=api_key), )\n",
        "\n",
        "    count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), \"cl100k_base\")\n",
        "\n",
        "    return search_index\n",
        "\n",
        "def answer(system, topic, temp = 1):\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": system},\n",
        "      {\"role\": \"user\", \"content\": topic}\n",
        "      ]\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=messages,\n",
        "      temperature=temp\n",
        "      )\n",
        "\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo-0301\":\n",
        "        num_tokens = 0\n",
        "        for message in messages:\n",
        "            num_tokens += 4\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value))\n",
        "                if key == \"name\":\n",
        "                    num_tokens += -1\n",
        "        num_tokens += 2\n",
        "        return num_tokens\n",
        "    else:\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
        "See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
        "\n",
        "\n",
        "def insert_newlines(text: str, max_len: int = 170) -> str:\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    current_line = \"\"\n",
        "    for word in words:\n",
        "        if len(current_line + \" \" + word) > max_len:\n",
        "            lines.append(current_line)\n",
        "            current_line = \"\"\n",
        "        current_line += \" \" + word\n",
        "    lines.append(current_line)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def answer_index(system, topic, search_index, temp=1, verbose=0):\n",
        "\n",
        "    docs = search_index.similarity_search(topic, k=5)\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "    message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\\nОтрывок документа №{i+1}\\n=====================' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))\n",
        "    if verbose: print('message_content :\\n ======================================== \\n', message_content)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system + f\"{message_content}\"},\n",
        "        {\"role\": \"user\", \"content\": topic}\n",
        "    ]\n",
        "\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "    if verbose: print(f\"{num_tokens_from_messages(messages, 'gpt-3.5-turbo-0301')} tokens used for the question\")\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=temp\n",
        "    )\n",
        "\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "    if verbose: print(f'{completion[\"usage\"][\"total_tokens\"]} total tokens used (question-answer).')\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "    answer = insert_newlines(completion.choices[0].message.content)\n",
        "    return answer\n",
        "\n",
        "def get_chatgpt_ansver3(system, topic, search_index, temp = 1):\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": topic}\n",
        "    ]\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=temp\n",
        "    )\n",
        "    print('ANSWER : \\n', insert_newlines(completion.choices[0].message.content))\n",
        "\n",
        "def answer_user_question(system_doc: str, knowledge_base_url: str, user_question: str) -> str:\n",
        "\n",
        "    system_doc_text = load_document_text(system_doc_url)\n",
        "    knowledge_base_text = load_document_text(knowledge_base_url)\n",
        "\n",
        "    # Создаем индексы поиска\n",
        "    knowledge_base_index = create_search_index(knowledge_base_text)\n",
        "\n",
        "    input_text = user_question\n",
        "\n",
        "    answer_text = answer_index(system_doc_text, input_text, knowledge_base_index, temp=temperature, verbose=verbose)\n",
        "\n",
        "    return answer_text"
      ],
      "metadata": {
        "id": "bY6haxQROhrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature=0.5\n",
        "verbose=0"
      ],
      "metadata": {
        "id": "vL7IXWuxOhua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инструкция для GPT, которая будет подаваться в system\n",
        "system_doc_url = 'https://docs.google.com/document/d/1JCp4OWP22EZTU-HllyanhYfNSy_RwJ1aP7dnXPY2hqE'"
      ],
      "metadata": {
        "id": "YNBOMnfnOhw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# База знаний, которая будет подаваться в LangChain\n",
        "knowledge_base_url = 'https://docs.google.com/document/d/1K8kIpWxUX8GNBtmPnStYy83zpV8zfVeBqWVVlH7wNdU' # база знаний"
      ],
      "metadata": {
        "id": "VlBLueNTOhy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вопрос пользователя\n",
        "topic= \"Как мне получить документы для предоставления в бухгалтерию моей компании после командировки?\""
      ],
      "metadata": {
        "id": "XY3BHthrOh1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans=answer_user_question(system_doc_url, knowledge_base_url, topic)\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "hfek3mlLOh4E",
        "outputId": "3064d432-696d-4d94-a622-67a0de224d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' При направлении вас в командировку документами, подтверждающими расходы по оплате билетов и/или услуг гостиницы, являются чеки контрольно-кассовой техники, слипы или\\n чеки электронных терминалов, если вы оплачивали услуги банковской картой, которая находится на ваше имя. Другие документы, такие как бумажные командировочные, мы не\\n предоставляем. Если вам нужны дополнительные документы или вы хотите бронировать командировки с полным пакетом закрывающих документов, пожалуйста, обратитесь в отдел по\\n работе с корпоративными клиентами на почту corporate@ostrovok.ru.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "THWmFU16Oh6J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}